{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIS4930 : Student Depression Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, r2_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depression_df = pd.read_csv('student_depression_prediction.csv')\n",
    "display(depression_df.head())\n",
    "display(depression_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(depression_df.info())\n",
    "display(depression_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "null_entries = depression_df.isnull().sum()\n",
    "print(\"Number of Nan:\")\n",
    "display(null_entries)\n",
    "\n",
    "# Check for duplicates\n",
    "num_duplicates = depression_df.duplicated().sum() # Sums duplicated entries\n",
    "print(\"Number of duplicate values: \", num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "depression_df.drop([\"id\"], axis=1).boxplot()\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df = depression_df.select_dtypes(include=[np.number]).drop(columns=['id'], axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation = numerical_df.corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', linewidths=0.5, fmt=\".3f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 6 different plots to explore the data\n",
    "fig, axes = plt.subplots(2, 3, figsize=(25, 20))\n",
    "\n",
    "# 1. Distribution of age by depression status\n",
    "sns.histplot(data=depression_df, x='Age', hue='Depression', multiple='stack', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Age Distribution by Depression Status')\n",
    "\n",
    "# 2. Relationship between Study Satisfaction and Depression\n",
    "sns.boxplot(data=depression_df, x='Depression', y='Study Satisfaction', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Study Satisfaction vs Depression')\n",
    "\n",
    "# 3. Sleep Duration vs Depression\n",
    "sns.countplot(data=depression_df, x='Sleep Duration', hue='Depression', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Sleep Duration vs Depression')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Academic Pressure by Depression Status\n",
    "sns.violinplot(data=depression_df, x='Depression', y='Academic Pressure', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Academic Pressure by Depression Status')\n",
    "\n",
    "# 5. Financial Stress vs Depression\n",
    "sns.boxplot(data=depression_df, x='Depression', y='Financial Stress', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Financial Stress vs Depression')\n",
    "\n",
    "# 6. Depression rate by Gender and Family History\n",
    "# 4. Academic Pressure by Depression Status\n",
    "sns.violinplot(data=depression_df, x='Depression', y='Study Satisfaction', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Study Satisfaction by Depression')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "numerical_df_copy = numerical_df.copy()\n",
    "numerical_df_copy.fillna(numerical_df_copy.mean(), inplace=True)\n",
    "\n",
    "def plot_categorical_dependency(df, feature_x, feature_y):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    if df[feature_x].nunique() < df[feature_y].nunique():\n",
    "        # X is more categorical — boxplot makes sense\n",
    "        sns.boxplot(x=feature_x, y=feature_y, data=df)\n",
    "        plt.title(f'{feature_y} by {feature_x}')\n",
    "    else:\n",
    "        # Flip if Y is more categorical\n",
    "        sns.boxplot(x=feature_y, y=feature_x, data=df)\n",
    "        plt.title(f'{feature_x} by {feature_y}')\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return (feature_x, feature_y)\n",
    "\n",
    "# Compare features' relationships with each other\n",
    "features_to_plot = [\n",
    "    'Age', 'Academic Pressure',\n",
    "    'Study Satisfaction', 'Work/Study Hours',\n",
    "    'Financial Stress', 'Depression'\n",
    "]\n",
    "\n",
    "results = []\n",
    "for feature_x in features_to_plot:\n",
    "    for feature_y in features_to_plot:\n",
    "        if feature_x != feature_y:\n",
    "            result = plot_categorical_dependency(numerical_df_copy, feature_x, feature_y)\n",
    "            results.append(result)\n",
    "\n",
    "for x, y in results:\n",
    "    print(f\"Plotted distribution of {y} grouped by {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def plot_feature_dependency(df, feature_x, feature_y, degree):\n",
    "    \n",
    "    degree = 2\n",
    "    # Fit polynomial regression\n",
    "    X = df[[feature_x]].values\n",
    "    y = df[feature_y].values\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "\n",
    "    # Predict line\n",
    "    y_pred = model.predict(X_poly)\n",
    "    slope = model.coef_[1]  # First polynomial coefficient\n",
    "    r2 = r2_score(y, y_pred)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(sorted(df[feature_x]), model.predict(poly.transform(sorted(df[feature_x].values.reshape(-1, 1)))), color='red', linewidth=1, label=f'Polynomial Fit (Degree: {degree}, Slope: {slope:.4f}, r2: {r2: .4f})')\n",
    "    plt.title(f\"{feature_y} vs {feature_x}\")\n",
    "    plt.xlabel(feature_x)\n",
    "    plt.ylabel(feature_y)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return (feature_x, feature_y, slope, r2)\n",
    "\n",
    "# Compare features relationship with depression\n",
    "features_to_plot = [\n",
    "    'Age' , 'Academic Pressure',\n",
    "    'Study Satisfaction', 'Work/Study Hours',\n",
    "    'Financial Stress', 'Depression'\n",
    "]\n",
    "results = []\n",
    "for feature_x in features_to_plot:\n",
    "    for feature_y in features_to_plot:\n",
    "        if feature_x != feature_y:\n",
    "            result = plot_feature_dependency(numerical_df_copy, feature_x, feature_y, degree=2)\n",
    "            results.append(result)\n",
    "\n",
    "# Print results\n",
    "for x, y, slope, r2 in results:\n",
    "    print(f\"{x} 'vs' {y} | Slope: {slope} | R²: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = depression_df.copy()\n",
    "\n",
    "# Drop nulls and duplicates\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Drop work pressure column for multicollinearity\n",
    "df = df.drop(columns=['Work Pressure'], axis=1)\n",
    "\n",
    "# Targeting Age, Work Pressure, CGPA, and Job Satisfaction for outliers\n",
    "target_cols = ['Age', 'CGPA', 'Job Satisfaction']\n",
    "original_len = len(df)\n",
    "\n",
    "for col in target_cols:\n",
    "  Q1 = df[col].quantile(0.25)\n",
    "  Q3 = df[col].quantile(0.75)\n",
    "  IQR = Q3 - Q1\n",
    "\n",
    "  lower_bound = Q1 - 1.5 * IQR\n",
    "  upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "  # Filter out outliers\n",
    "  df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "# Show how many rows were removed\n",
    "removed_rows = original_len - len(df)\n",
    "print(f\"Original dataset size: {original_len}\")\n",
    "print(f\"Cleaned dataset size: {len(df)}\")\n",
    "print(f\"Removed {removed_rows} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['Gender', 'City', 'Profession', 'Sleep Duration', 'Dietary Habits', 'Degree', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']\n",
    "for column in categorical_columns:\n",
    "    df[column] = LabelEncoder().fit_transform(df[column])\n",
    "\n",
    "X = df.drop(['id', 'Depression'], axis=1)\n",
    "y = df['Depression']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View if there is class imbalance\n",
    "class_counts = y.value_counts().to_frame()\n",
    "class_counts.index = [\"Positive\", \"Negative\"]\n",
    "display(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=16)\n",
    "\n",
    "(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to the training data only\n",
    "smote = SMOTE(random_state=16)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Visualize class distribution before and after SMOTE\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Before SMOTE\n",
    "sns.countplot(x=y_train, ax=axes[0])\n",
    "axes[0].set_title(\"Class Distribution Before SMOTE\")\n",
    "axes[0].set_xlabel(\"Class\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# After SMOTE\n",
    "sns.countplot(x=y_train_resampled, ax=axes[1])\n",
    "axes[1].set_title(\"Class Distribution After SMOTE\")\n",
    "axes[1].set_xlabel(\"Class\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the new class distribution\n",
    "print(\"Before SMOTE:\", pd.Series(y_train).value_counts())\n",
    "print(\"After SMOTE:\", pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, name, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1 Score: \", f1)\n",
    "    print()\n",
    "\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", cbar=False)\n",
    "    plt.title(f\"Confusion Matrix - {name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xticks([0.5, 1.5], [\"No Depression\", \"Depression\"])\n",
    "    plt.yticks([0.5, 1.5], [\"Depression\", \"Depression\"])\n",
    "    plt.show()\n",
    "\n",
    "    return (accuracy, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, random_state=16)\n",
    "logistic_model.fit(X_train_resampled, y_train_resampled)\n",
    "logistic_result = evaluate_model(logistic_model, \"Logistic Regression\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_values = {\"n_neighbors\": list(range(1, 20, 2)),\n",
    "                    \"weights\": [\"uniform\", \"distance\"],\n",
    "                    \"p\": [1, 1.5, 2], \"n_jobs\": [5]\n",
    "                   }\n",
    "gs_knn = KNeighborsClassifier()\n",
    "gs = GridSearchCV(gs_knn, parameter_values, scoring=\"f1\", n_jobs=-1)\n",
    "gs.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(\"Best parameters: \", gs.best_params_)\n",
    "\n",
    "knn_model = KNeighborsClassifier(**gs.best_params_)\n",
    "knn_model.fit(X_train_resampled, y_train_resampled)\n",
    "knn_result = evaluate_model(knn_model, \"KNN\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train and evaluate a Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(    \n",
    "    class_weight='balanced',\n",
    "    criterion='entropy',\n",
    "    max_depth=10,\n",
    "    max_leaf_nodes=None,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=2,\n",
    "    random_state=1217\n",
    "    )\n",
    "dt_model.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"Decision Tree Model Evaluation:\")\n",
    "evaluate_model(dt_model, \"Decision Tree\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "estimator = DecisionTreeClassifier()\n",
    "bag_model = BaggingClassifier(\n",
    "    estimator=estimator,\n",
    "    n_estimators=100,\n",
    "    max_samples=0.8,\n",
    "    bootstrap=True,\n",
    "    random_state=295\n",
    ")\n",
    "\n",
    "bag_model.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"Bagging Model Evaluation:\")\n",
    "evaluate_model(bag_model, \"Bagging\", X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "estimator = DecisionTreeClassifier(max_depth=1)\n",
    "adc = AdaBoostClassifier(estimator=estimator, n_estimators=100, learning_rate=1)\n",
    "adc.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"AdaBoost Model Evaluation:\")\n",
    "evaluate_model(adc, \"AdaBoost\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "base_models = [(\"Decision Tree\", DecisionTreeClassifier()), (\"KNN\", KNeighborsClassifier())]\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "sc = StackingClassifier(base_models, meta_model)\n",
    "sc.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"Stacking Model Evaluation:\")\n",
    "evaluate_model(sc, \"Stacking\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train and evaluate a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=1217)\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"Random Forest Model Evaluation:\")\n",
    "evaluate_model(rf_model, \"Random Forest\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train and evaluate a Support Vector Machine (SVM) model\n",
    "svm_model = SVC(kernel='linear', random_state=1217)\n",
    "svm_model.fit(X_train_resampled, y_train_resampled)\n",
    "print(\"SVM Model Evaluation:\")\n",
    "evaluate_model(svm_model, \"SVM\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Function to build a simple baseline model\n",
    "def build_baseline_model(input_shape):\n",
    "    model = Sequential(name='Baseline')\n",
    "    model.add(Dense(32, activation='relu', input_shape=input_shape))  # Single hidden layer\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "    return model\n",
    "\n",
    "# Function to build a deeper neural network model\n",
    "def build_deep_model(input_shape):\n",
    "    model = Sequential(name='Deep')\n",
    "    model.add(Dense(64, activation='relu', input_shape=input_shape))  # First hidden layer\n",
    "    model.add(Dropout(0.3))  # Dropout for regularization\n",
    "    model.add(Dense(32, activation='relu'))  # Second hidden layer\n",
    "    model.add(Dense(16, activation='relu'))  # Third hidden layer\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "    return model\n",
    "\n",
    "# Function to build a wider neural network model\n",
    "def build_wide_model(input_shape):\n",
    "    model = Sequential(name='Wide')\n",
    "    model.add(Dense(128, activation='relu', input_shape=input_shape))  # Wide hidden layer\n",
    "    model.add(Dropout(0.4))  # Dropout for regularization\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "    return model\n",
    "\n",
    "def build_model(name, input_shape):\n",
    "    if name == 'Baseline':\n",
    "        return build_baseline_model(input_shape)\n",
    "    elif name == 'Deep':\n",
    "        return build_deep_model(input_shape)\n",
    "    elif name == 'Wide':\n",
    "        return build_wide_model(input_shape)\n",
    "\n",
    "# List of model types to train\n",
    "models = ['Baseline', 'Deep', 'Wide']\n",
    "\n",
    "# Loop through each model type, train, and evaluate\n",
    "for name in models:\n",
    "    print(f\"\\n Training {name} Model\")\n",
    "\n",
    "    # Build the model\n",
    "    model = build_model(name, (X_train_resampled.shape[1],))\n",
    "\n",
    "    # Compile the model with binary crossentropy loss and relevant metrics\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "    # Define early stopping callback to prevent overfitting\n",
    "    early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_resampled, y_train_resampled,\n",
    "                        validation_split=0.2,\n",
    "                        epochs=100,\n",
    "                        batch_size=32,\n",
    "                        callbacks=[early_stop],\n",
    "                        verbose=0)\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    print(\"Evaluating on Test Set:\")\n",
    "    results = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(dict(zip(model.metrics_names, results)))  # Display metrics\n",
    "\n",
    "    # Make predictions\n",
    "    y_probs = model.predict(X_test)  # Probabilities\n",
    "    y_pred = (y_probs > 0.5).astype(int)  # Convert probabilities to class labels\n",
    "\n",
    "    # Display classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Display confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title(f'{name} - Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # Function to plot training history for each metric\n",
    "    def plot_history(history, metric):\n",
    "        plt.plot(history.history[metric], label=f'{metric} (train)')\n",
    "        plt.plot(history.history[f'val_{metric}'], label=f'{metric} (test)')\n",
    "        plt.title(f'{name} - {metric.capitalize()} over Epochs')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Metrics: \\n\")\n",
    "    # Plot all relevant metrics\n",
    "    for metric in ['accuracy', 'loss', 'precision', 'recall']:\n",
    "        plot_history(history, metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBClassifier\n",
    "# RFE with XGboost\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=['id', 'Depression'])\n",
    "y = df['Depression']\n",
    "\n",
    "# Scale features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Wrapper method: Recursive Feature Elimination (RFE) with XGBoost\n",
    "base_model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "rfe = RFE(estimator=base_model, n_features_to_select=10)\n",
    "X_selected = rfe.fit_transform(X_scaled, y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit model on selected features\n",
    "base_model.fit(X_train, y_train)\n",
    "y_pred = base_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"XGBoost with RFE-selected Features\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"Not Depressed\", \"Depressed\"],\n",
    "            yticklabels=[\"Not Depressed\", \"Depressed\"])\n",
    "plt.title(\"Confusion Matrix - RFE + XGBoost\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Chi squared feature selection with SVM\n",
    "\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=['id', 'Depression'])\n",
    "y = df['Depression']\n",
    "\n",
    "# Chi2 requires non-negative data → use MinMaxScaler\n",
    "X_scaled = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Select top 10 features using Chi-Squared test\n",
    "selector = SelectKBest(score_func=chi2, k=10)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM model\n",
    "svm_model = SVC(kernel='linear')  # You can also try 'rbf'\n",
    "svm_model.fit(X_train, y_train)\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Chi-Squared Feature Selection + SVM\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"Not Depressed\", \"Depressed\"],\n",
    "            yticklabels=[\"Not Depressed\", \"Depressed\"])\n",
    "plt.title(\"Confusion Matrix - Chi2 + SVM\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L1 Lasso regularization with logistic regression\n",
    "\n",
    "# Prepare data\n",
    "X = df.drop(columns=['id', 'Depression'])\n",
    "y = df['Depression']\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# L1 Logistic Regression for feature selection\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# Select features where coefficients are non-zero\n",
    "import numpy as np\n",
    "selected_mask = model.coef_[0] != 0\n",
    "X_selected = X_scaled[:, selected_mask]\n",
    "\n",
    "# Split data using only selected features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Retrain on selected features\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Embedded Method (L1 Logistic Regression)\")\n",
    "print(\"Selected features:\", np.array(X.columns)[selected_mask])\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"Not Depressed\", \"Depressed\"],\n",
    "            yticklabels=[\"Not Depressed\", \"Depressed\"])\n",
    "plt.title(\"Confusion Matrix - L1 Logistic Regression\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
